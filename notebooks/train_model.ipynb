{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d276e9f",
   "metadata": {},
   "source": [
    "# Training the RoBERTa Model\n",
    "In this notebook, we:\n",
    "- Wrap tokenized data into PyTorch datasets\n",
    "- Load a pre-trained `roberta-base` model\n",
    "- Fine-tune it on our labeled fake news dataset\n",
    "- Evaluate performance using accuracy and F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ca1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, get_scheduler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce108004",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/train_encodings.pkl\", \"rb\") as f:\n",
    "    train_encodings = pickle.load(f)\n",
    "with open(\"artifacts/train_labels.pkl\", \"rb\") as f:\n",
    "    train_labels = pickle.load(f)\n",
    "with open(\"artifacts/val_encodings.pkl\", \"rb\") as f:\n",
    "    val_encodings = pickle.load(f)\n",
    "with open(\"artifacts/val_labels.pkl\", \"rb\") as f:\n",
    "    val_labels = pickle.load(f)\n",
    "with open(\"artifacts/test_encodings.pkl\", \"rb\") as f:\n",
    "    test_encodings = pickle.load(f)\n",
    "with open(\"artifacts/test_labels.pkl\", \"rb\") as f:\n",
    "    test_labels = pickle.load(f)\n",
    "with open(\"artifacts/train_df.pkl\", \"rb\") as f:\n",
    "    train_df = pickle.load(f)\n",
    "with open(\"artifacts/val_df.pkl\", \"rb\") as f:\n",
    "    val_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c186a",
   "metadata": {},
   "source": [
    "## Create Custom Dataset Class\n",
    "\n",
    "We need to wrap the `input_ids`, `attention_mask`, and `labels` into a single PyTorch `Dataset` object.  \n",
    "This makes it easy to iterate over batches during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4780e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb19048",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "val_dataset = NewsDataset(val_encodings, val_labels)\n",
    "test_dataset = NewsDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19df3b",
   "metadata": {},
   "source": [
    "## Load Pretrained RoBERTa Model with Classification Head\n",
    "\n",
    "Weâ€™ll now load `roberta-base` and add a classification head for **binary classification (2 labels)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fc684",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cec551",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281fb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9fb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train label distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print(\"Validation label distribution:\")\n",
    "print(val_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a0898",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We'll train the model for a few epochs:\n",
    "- Use loss.backward() for gradient calculation\n",
    "- Use optimizer.step() to update weights\n",
    "- Track loss and evaluation metrics per epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25274275",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = torch.tensor([train_df['label'].value_counts()[0] / train_df['label'].value_counts()[1]]).to(device)\n",
    "\n",
    "criterion = BCEWithLogitsLoss(pos_weight=pos_weight)  # Handles class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = torch.tensor([18785, 17133], dtype=torch.float)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # <- Make sure the model is on the same device\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(train_loader))\n",
    "\n",
    "    # Only move tensor items to the device\n",
    "    batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = model(**batch)\n",
    "    end_time = time.time()\n",
    "\n",
    "print(f\"Single forward pass time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "epochs = 10\n",
    "\n",
    "# Optional scheduler to reduce LR on plateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            # Let model compute loss itself\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        progress_bar.set_postfix(loss=loss.item(), avg_loss=avg_loss)\n",
    "\n",
    "        if epoch == 0 and batch_idx == 0:\n",
    "            print(\"\\n Batch Tensor Devices (first batch only):\")\n",
    "            for k, v in batch.items():\n",
    "                print(f\"{k}: shape={v.shape}, dtype={v.dtype}, device={v.device}\")\n",
    "\n",
    "    print(f\"\\n Epoch {epoch + 1} - Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(**batch).logits\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()  # Softmax for multi-class\n",
    "\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "\n",
    "        pred_labels.extend(preds)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "# Results\n",
    "f1 = f1_score(true_labels, pred_labels, average=\"macro\")\n",
    "print(f\"\\nâœ… Validation F1 Score: {f1:.4f}\")\n",
    "print(\"ðŸ” Predicted label distribution:\", np.unique(pred_labels, return_counts=True))\n",
    "print(\"ðŸ” True label distribution:\", np.unique(true_labels, return_counts=True))\n",
    "print(\"\\nðŸ“Š Classification Report:\\n\", classification_report(true_labels, pred_labels, target_names=['fake', 'real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"../model\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer (make sure you're using the same one you loaded before)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
